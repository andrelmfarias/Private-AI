{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with PySyft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T12:31:32.156640Z",
     "start_time": "2019-08-06T12:31:31.473424Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Pysyft and setting virtual workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T12:31:41.172976Z",
     "start_time": "2019-08-06T12:31:32.473812Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0806 14:31:40.023433 4716291520 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/Users/andre.farias/pysyft-env/lib/python3.7/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
      "W0806 14:31:40.080029 4716291520 deprecation_wrapper.py:119] From /Users/andre.farias/pysyft-env/lib/python3.7/site-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0806 14:31:41.170000 4716291520 base.py:646] Worker me already exists. Replacing old worker which could cause                     unexpected behavior\n"
     ]
    }
   ],
   "source": [
    "import syft as sy  # import the Pysyft library\n",
    "\n",
    "# hook PyTorch to add extra functionalities like Federated and Encrypted Learning\n",
    "hook = sy.TorchHook(torch) \n",
    "\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "james = sy.VirtualWorker(hook, id=\"james\")\n",
    "jon = sy.VirtualWorker(hook, id=\"james\").add_worker(sy.local_worker)\n",
    "\n",
    "workers = [bob, alice, james]\n",
    "crypto_provider = jon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating simulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the objective of this tutorial is to simulated data for a Linear Regression, I will generate data folowing the linear model assumption:\n",
    "\n",
    "$$ y = \\beta  X + \\epsilon $$\n",
    "\n",
    "Where $ X $ is a matrix that corresponds to the independent input variables or features, $ y $ corresponds to the output variable that depends on the inputs, $ \\beta $ are the coefficients we are trying to model and $ \\epsilon $ is  the noise that we suppose to be drawn from a normal distribution with mean 0.\n",
    "\n",
    "For this simulated data I will suppose:\n",
    "- There are 300 samples distributed equally across 3 workers\n",
    "- There are 10 independent variable (or features)\n",
    "- The \"real\" coefficients are $\\beta = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T12:31:41.304592Z",
     "start_time": "2019-08-06T12:31:41.293747Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 300 # number of samples\n",
    "K = 10 # number of features\n",
    "\n",
    "X = torch.rand((N, K))*10 # simulated input\n",
    "beta = torch.Tensor(np.linspace(1, 10, 10)).view(-1,1) # \"real\" coefficients\n",
    "eps = torch.randn((N,1)) # simulated noise\n",
    "\n",
    "y = torch.matmul(X, beta) + eps # simulated output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the objective is to simulated distributed data on 3 workers and solve the linear regression problem to obtain the best estimator $\\hat{\\beta}$.\n",
    "\n",
    "For this I will first send a different chunk of data for each worker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T12:31:46.691669Z",
     "start_time": "2019-08-06T12:31:46.674471Z"
    }
   },
   "outputs": [],
   "source": [
    "chunk_size = int(N/len(workers))\n",
    "X_chunks = []\n",
    "y_chunks = []\n",
    "sizes = []\n",
    "for i, worker in enumerate(workers):\n",
    "    X_chunks.append(X[i*chunk_size:(i+1)*chunk_size].send(worker))\n",
    "    y_chunks.append(y[i*chunk_size:(i+1)*chunk_size].send(worker))\n",
    "    sizes.append(torch.Tensor([chunk_size]).send(worker))\n",
    "    \n",
    "# Keeping the pointers to each chunk in a MultiPointerTensor\n",
    "X_mpt = sy.MultiPointerTensor(children=X_chunks)\n",
    "y_mpt = sy.MultiPointerTensor(children=y_chunks)\n",
    "sizes_mpt = sy.MultiPointerTensor(children=sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Linear Regression with distributed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the simulated environment set up with the data virtually distributed among three different servers, let's train our linear model in a distributed manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Plaintext version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first and simpler way to solve the Linear Regression problem with distributed data does not need encrypted computation. The main idea is to separate the solution in two stages:\n",
    "\n",
    "- **Compress**: compute the following aggregates separately in each server:\n",
    "\n",
    "$$ y^T y, \\;\\; X^T y, \\;\\; X^T X $$\n",
    "\n",
    "- **Combine**: cumpute the matrices aggregates and $N$ for the whole model by summing the aggregates between workers and the sample size of each worker, and then  obtain $\\hat{\\beta}$ and its standard error from it\n",
    "\n",
    "When combining the aggregates without encrypted computation we leak some information about the data. However it is important to note that when we have large amounts of data, thanks to the **compress** stage, it's very difficult to revert these aggregates to the original data. \n",
    "\n",
    "Moreover, thanks to this two-stage process we are able to perform a Linear Regression in a distributed manner while speeding the computation up: for big $N$ and small $K$ we reduce the computation time by performing **compression** in parallel (which has complexity $\\mathcal{O}(NK^2 / n_{workers})$ ) and then performing **combination** (which  has complexity $\\mathcal{O}(K^3)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compress stage: computing aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T12:31:49.569788Z",
     "start_time": "2019-08-06T12:31:49.544436Z"
    }
   },
   "outputs": [],
   "source": [
    "# Computing matrices aggregates multipointers\n",
    "yTy_mpt = y_mpt.transpose(0,1).mm(y_mpt)\n",
    "XTy_mpt = X_mpt.transpose(0,1).mm(y_mpt)\n",
    "XTX_mpt = X_mpt.transpose(0,1).mm(X_mpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine stage: summing aggregates and solving Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T12:31:50.129387Z",
     "start_time": "2019-08-06T12:31:50.112085Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sending aggregates to local worker using the method .get() and \n",
    "# summing them to obtain the whole aggregates\n",
    "yTy = sum(yTy_mpt.get())\n",
    "XTy = sum(XTy_mpt.get())\n",
    "XTX = sum(XTX_mpt.get())\n",
    "N = sum(sizes_mpt.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain the mean $\\hat{\\beta}$ we need to solve the following linear system:\n",
    "\n",
    "$$ (X^TX)\\hat{\\beta} = X^T y $$\n",
    "\n",
    "The variance of $\\beta$ is given by:\n",
    "\n",
    "$$ (X^TX)^{-1} \\sigma^2 $$\n",
    "\n",
    "Where sigma is obtained from its unbiased estimator:\n",
    "\n",
    "$$ \\hat{\\sigma}^2 = \\frac{y^Ty - \\hat{\\beta}^T (X^TX)\\hat{\\beta}}{N - K} $$ \n",
    "\n",
    "For this version of the solution (plaintext with no encrypted computation) we will only need a library to compute linear algebra operations such as linear system solving and matrix inversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T12:31:50.698768Z",
     "start_time": "2019-08-06T12:31:50.689000Z"
    }
   },
   "outputs": [],
   "source": [
    "# Solving with Linear Algebra\n",
    "yTy = np.array(yTy)\n",
    "XTy = np.array(XTy)\n",
    "XTX = np.array(XTX)\n",
    "N = np.array(N)\n",
    "\n",
    "beta_hat = np.linalg.solve(XTX, XTy)\n",
    "\n",
    "sig2_hat = (yTy - beta_hat.T @ XTX @ beta_hat) / (N - K)\n",
    "\n",
    "var = np.linalg.inv(XTX) * sig2_hat\n",
    "\n",
    "beta_std = np.sqrt(np.diag(var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's display our results and compare with the \"real\" values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T12:31:51.768940Z",
     "start_time": "2019-08-06T12:31:51.753405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients | Standard error\n",
      "-----------------------------\n",
      "    0.986    |     0.020\n",
      "    2.017    |     0.019\n",
      "    3.038    |     0.020\n",
      "    3.990    |     0.019\n",
      "    4.998    |     0.019\n",
      "    5.990    |     0.021\n",
      "    6.976    |     0.019\n",
      "    7.991    |     0.020\n",
      "    9.016    |     0.020\n",
      "    9.997    |     0.019\n",
      "-----------------------------\n",
      "   Noise variance: 1.034\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "def display_results(coeffs, std_errors, sigma2):\n",
    "    print(\"Coefficients | Standard error\")\n",
    "    print(\"-----------------------------\")\n",
    "    for coef, std in zip(coeffs.squeeze(), std_errors):\n",
    "        print(\"    {:.3f} \".format(coef),\"  | \", \"   {:.3f}\".format(std))\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"   Noise variance: {:.3f}\".format(sigma2.squeeze()))\n",
    "    print(\"-----------------------------\")\n",
    "    \n",
    "display_results(beta_hat, beta_std, sig2_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice our computed coefficients are close to the \"real\" ones, and the variance of noise is close to 1 as expected."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
